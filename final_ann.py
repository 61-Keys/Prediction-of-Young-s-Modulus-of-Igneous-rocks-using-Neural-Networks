# -*- coding: utf-8 -*-
"""Final ANN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L5WlNpmfGb1D-tSizswFN7wwyatdl-V8
"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.neural_network import MLPRegressor
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
from sklearn import linear_model
from sklearn.linear_model import LinearRegression
from sklearn import metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split, cross_val_score

df = pd.read_csv("/content/final.csv")

df.head()

df.shape

print(df.corr())

correlation_matrix = df.corr()


plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Heatmap')
plt.show()

print(df.describe())

import seaborn as sns
import matplotlib.pyplot as plt

# Create a DataFrame
data = {
    'Input 1': [283.44, 152.81, 35.00, 175.00, 300.00, 400.00, 500.00],
    'Input 2': [72.58, 32.36, 9.00, 45.75, 72.00, 93.25, 138.00],
    'Input 3': [10.27, 2.83, 4.00, 8.00, 10.00, 12.00, 17.00],
    'Input 4': [4316.94, 1233.08, 1955.00, 3304.75, 4104.50, 5554.00, 5965.00],
    'Input 5': [2565.29, 92.26, 2391.00, 2521.00, 2556.50, 2600.25, 2884.00],
    'Input 6': [0.0946, 0.4359, -0.1714, 0.0000, 0.0102, 0.0489, 2.8686],
    'Output': [36.04, 21.28, 7.00, 23.50, 30.00, 45.75, 109.00]
}
df = pd.DataFrame(data)

# Set the style of seaborn
sns.set(style="whitegrid")

# Create subplots for each variable
plt.figure(figsize=(12, 6))
for i, column in enumerate(df.columns):
    plt.subplot(2, 4, i+1)
    sns.boxplot(x=df[column])
    plt.title(column)
    plt.tight_layout()

plt.show()

X = df.drop(columns = ['Output'], axis = 1)
y = df['Output']

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

def train_ann(X_train, y_train):
    ann = MLPRegressor(hidden_layer_sizes=(100,), activation='logistic', solver='lbfgs', max_iter=1000, random_state=42)
    ann.fit(X_train, y_train)
    return ann

def feed_network(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    ann = train_ann(X_train_scaled, y_train)

    y_pred_train = ann.predict(X_train_scaled)
    y_pred_test = ann.predict(X_test_scaled)

    return y_pred_train, y_train, y_pred_test, y_test, ann

def evaluate_performance(y_true, y_pred):
    correlation = np.corrcoef(y_true, y_pred)[0, 1]
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    return correlation, rmse

def test_ann(ann, X_test, y_test):
    scaler = StandardScaler()
    X_test_scaled = scaler.fit_transform(X_test)
    y_pred_test = ann.predict(X_test_scaled)
    return y_pred_test, y_test

def evaluate_test_performance(y_true, y_pred):
    correlation, rmse, r_squared = evaluate_performance(y_true, y_pred)
    return correlation, rmse, r_squared

# Calling the functions sequentially
print("\nTraining ANN with one hidden layer and 100 neurons:")
y_pred_train, y_train_actual, y_pred_test, y_test_actual, ann = feed_network(X_train, y_train)

def evaluate_performance(y_true, y_pred):
    correlation = np.corrcoef(y_true, y_pred)[0, 1]
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r_squared = r2_score(y_true, y_pred)
    return correlation, rmse, r_squared

correlation_train, rmse_train, r_squared_train = evaluate_performance(y_train_actual, y_pred_train)
print("Performance on Training Data:")
print("Correlation Coefficient:", correlation_train)
print("Root Mean Squared Error (RMSE):", rmse_train)
print("R-squared:", r_squared_train)

print("\nPerformance on Validation Data:")
correlation_val, rmse_val, r_squared_val = evaluate_test_performance(y_val_actual, y_pred_val)
print("Correlation Coefficient:", correlation_val)
print("Root Mean Squared Error (RMSE):", rmse_val)
print("R-squared:", r_squared_val)

print("\nPerformance on Testing Data:")
correlation_test, rmse_test, r_squared_test = evaluate_test_performance(y_test_actual, y_pred_test)
print("Correlation Coefficient:", correlation_test)
print("Root Mean Squared Error (RMSE):", rmse_test)
print("R-squared:", r_squared_test)

"""### **Analyzing R^2 Values with Varying Neuron Count in MLP Regression**

"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from sklearn.neural_network import MLPRegressor
import pandas as pd

# Provided input parameters and output parameter
input_parameters = ['Input 1', 'Input 2', 'Input 3', 'Input 4', 'Input 5', 'Input 6']
output_parameter = 'Output'

# Assuming X contains input parameters and y contains output (target variable)
# Replace X and y with your actual data
# X = your_input_data
# y = your_output_data

# Training
def train_ann(X_train, y_train, neurons):
    ann = MLPRegressor(hidden_layer_sizes=(neurons,), activation='logistic', solver='lbfgs', max_iter=1000, random_state=42)
    ann.fit(X_train, y_train)
    return ann

# Function to feed the network with training data
def feed_network(X, y, neurons):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    ann = train_ann(X_train_scaled, y_train, neurons)

    y_pred_test = ann.predict(X_test_scaled)

    return y_test, y_pred_test

# Create a table of R^2 values for different numbers of neurons
neurons_list = list(range(1, 101))
r2_values = []

for neurons in neurons_list:
    y_actual, y_pred = feed_network(X, y, neurons)
    r2 = r2_score(y_actual, y_pred)
    r2_values.append(r2)

# Create a DataFrame for the table
table_data = {
    'Number of Neurons': neurons_list,
    'R^2 Values': r2_values
}

table_df = pd.DataFrame(table_data)

# Display the table
print(table_df)

# Find the row with the highest R^2 value
max_r2_row = table_df[table_df['R^2 Values'] == table_df['R^2 Values'].max()]

# Print the row
print("Row with the highest R^2 value:")
print(max_r2_row)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import r2_score

# Generate synthetic data
np.random.seed(0)
X = np.random.rand(100, 1)
y = 2 * X.squeeze() + np.random.randn(100)  # Linear relationship with noise

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a neural network model
model = MLPRegressor(hidden_layer_sizes=(10,), activation='relu', max_iter=1000, random_state=42)
model.fit(X_train, y_train)

# Predict on test data
y_pred = model.predict(X_test)

# Calculate R-squared
r_squared = r2_score(y_test, y_pred)
print("R-squared:", r_squared)

# Plot the data and predictions
plt.figure(figsize=(8, 6))
plt.scatter(X_test, y_test, color='blue', label='Actual')
plt.scatter(X_test, y_pred, color='red', label='Predicted')
plt.title('Neural Network Regression')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()

